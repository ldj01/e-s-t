#! /usr/bin/env python3

'''
    PURPOSE: Retrieves archived NARR files from the NCEP for the dates
             requested.  Extracts the variables ST requires (HGT, TMP, SPFH)
             and repackages them into our internal location and filenames.

    PROJECT: Land Satellites Data Systems Science Research and Development
             (LSRD) at the USGS EROS

    LICENSE: NASA Open Source Agreement 1.3

    NOTES:

          NCEP     - National Centers for Environmental Prediction
                     http://www.ncep.noaa.gov

          NARR     - NCEP North American Regional Reanalysis

          CISL RDA - Computational & Information Systems Lab
                     Research Data Archive http://rda.ucar.edu

          NCAR     - National Center for Atmospheric Research
                     http://ncar.ucar.edu

          UCAR     - University Corporation for Atmospheric Research
                     http://www2.ucar.edu
'''

import os
import shutil
import sys
import logging
from datetime import datetime, timedelta

from espa.auxiliary.aux_utilities import System, SUCCESS, ERROR
from espa import AuxConfig
from espa import HttpSession
import cx_Oracle

# Global variables
NARR_VARIABLES = ['HGT', 'SPFH', 'TMP']
logger = logging.getLogger(__name__)

def get_date_times(s_date, e_date, cfg, overwrite, archive):
    """ Generator to iterate through dates in 3 hour time periods. If the
        overwrite flag is set, all dates between s_date and e_date will be
        returned.  If overwrite is not set, the archive list will be used to
        determine if a datetime already has archive data which means it will not
        be returned.

        Parameters:
            s_date: Datetime of the first date to consider processing
            e_date: Datetime of the last date to consider processing
            cfg: Config information to determine filenames and paths
            overwrite: Flag to determine if the existing archive data is to be
                overwritten or skipped
            archive: List of files in the current archive
            interval: Timedelta of the amount of time to increment by between
                s_date and e_date

        Returns:
            Generator of all datetimes between s_date and e_date incremented by
            interval and skips previously archived dates if overwrite is set
    """

    logger.info('Generating list of data from %s to %s', s_date, e_date)

    cur_time = datetime.combine(s_date.date(), datetime.min.time())
    end_time = datetime.combine(e_date.date(), datetime.max.time())

    while cur_time <= end_time:
        # Generate the filename and path for this specific date to check against
        arcdir = cfg.get('narr_base_archive')
        arcdir = cfg.get('narr_archive_format').format(arcdir, cur_time.year,
                                                cur_time.month, cur_time.day)
        narr_file = cfg.get('narr_filename_format').format('HGT', cur_time.year,
                        cur_time.month, cur_time.day, cur_time.hour*100, 'grb')
        # If we already have the file for this date and time, skip it
        if overwrite or os.path.join(arcdir, narr_file) not in archive:
            yield cur_time

        cur_time += timedelta(hours=3)

def archive_aux_data(cfg, dates, overwrite, archive, con, dbh):
    """ Iterates through the dates generated by get_date_time and
        downloads, processes, and archives the dates given.

        Parameters:
            cfg: Config information to determine filenames and paths
            dates: Generator of datetimes to gather archive data for
            overwrite: Flag to determine if the existing archive data is to be
                overwritten or skipped
            archive: List of files in the current archive
            con: Connection to database
            dbh: Handle to interact with the database

        Returns:
            SUCCESS/ERROR on successful or failed processing
    """
    # Create an HttpSession object for downloading
    session = HttpSession()

    for date in dates:
        # Create the download directory if it doesn't exist
        dload_dir = cfg.get('narr_temp_directory')
        System.create_directory(dload_dir)

        # Generate the download url and local file location
        online_file = cfg.get('ncep_filename_format').format(date.year,
                              date.month, date.day, date.hour)
        url = cfg.get('ncep_url_format').format(online_file)
        # Keep the online file name when downloading to the temp directory
        dload_file = os.path.join(dload_dir, online_file)

        # Download the file to a temporary download directory
        status = session.http_transfer_file(url, dload_file)
        if not status:
            logger.warn('Unsuccessful download of %s to %s. '
                        'Continue processing', url, dload_file)
            continue

        # Process the HGT, SPFH, and TMP variables out of the downloaded file
        logger.info("Processing [%s]", dload_file)
        # Create variables to keep track of new archive files and db command
        new_archive = []
        values = {'coverage' : date}

        for var in NARR_VARIABLES:
            # Generate the grib and hdr filenames
            hdr_name = cfg.get('narr_filename_format').format(var, date.year,
                               date.month, date.day, date.hour*100, 'hdr')
            grib_name = cfg.get('narr_filename_format').format(var, date.year,
                                date.month, date.day, date.hour*100, 'grb')
            hdr_file = os.path.join(dload_dir, hdr_name)
            grib_file = os.path.join(dload_dir, grib_name)

            # Create inventory/header file to extract the variable data
            create_temp_header = ['wgrib', dload_file, '|', 'grep', var,
                                  '>', hdr_file]
            # Create grib files for each variable
            create_final_grib = ['cat', hdr_file, '|', 'wgrib', dload_file,
                                 '-i', '-grib', '-o', grib_file]
            # Create new inventory/header file for the variable
            create_final_header = ['wgrib', grib_file, '|', 'grep', var,
                                   '>', hdr_file]

            cmds = [create_temp_header, create_final_grib, create_final_header]

            # Execute the variable extraction commands
            for cmd_list in cmds:
                cmd = ' '.join(cmd_list)
                status = System.execute_cmd(cmd)
                if status != 0:
                    logger.error('Error executing command [%s].', cmd)
                    return ERROR

            new_archive.append(grib_name)
            new_archive.append(hdr_name)
            values[var + '_grib'] = grib_name
            # End var loop

        # Create the archive directory if it doesn't exist
        arcdir = cfg.get('narr_base_archive')
        arcdir = cfg.get('narr_archive_format').format(arcdir, date.year,
                                                       date.month, date.day)
        System.create_directory(arcdir)

        # After successful variable extraction, move the new archive files
        for f in new_archive:
            shutil.move(os.path.join(dload_dir, f), os.path.join(arcdir, f))

        # Update the database for the new files
        try:
            # Assume that if one file was in the archive, the group should also
            # be there
            existing = overwrite and os.path.join(arcdir,
                                                  new_archive[0]) in archive

            if existing:
                # If we are overwriting and the file is already in the archive
                # then a db record already exists and must be updated
                logger.info('Updating existing database archive entry')
                dbh.execute('UPDATE NARR SET FILE_NAME_HEIGHT = :HGT_grib, '
                            'FILE_NAME_HUMIDITY = :SPFH_grib, FILE_NAME_TEMP = '
                            ':TMP_grib, DATE_ENTERED = SYSDATE WHERE '
                            'EFFECTIVE_DATE_TIME = :coverage', values)
                if dbh.rowcount != 1:
                    logger.error('No rows updated')
                    raise cx_Oracle.DatabaseError
            else:
                logger.info('Creating new database archive entry')
                dbh.execute('INSERT INTO NARR (EFFECTIVE_DATE_TIME, '
                            'FILE_NAME_HEIGHT, FILE_NAME_HUMIDITY, '
                            'FILE_NAME_TEMP, DATE_ENTERED) VALUES (:coverage, '
                            ':HGT_grib, :SPFH_grib, :TMP_grib, SYSDATE)',values)
            con.commit()
            logger.info('Processing successful for %s', date)
        except cx_Oracle.DatabaseError as e:
            logger.error('Database entry for %s has failed: %s'
                         'Exiting processing and cleaning the archive', date, e)
            # If the file was not already in the archive, remove it
            if not existing:
                logger.error('Removing the archived files to match db records')
                for fname in new_archive:
                    os.remove(os.path.join(arcdir, fname))
            else:
                logger.error('These files previously existed in the local '
                             'archive. Double check that the archive and '
                             'database records for these files match.')

            return ERROR

        # Clean the temp directory
        System.empty_directory(dload_dir)
    # End for date loop

    return SUCCESS


def main():
    """ Main routine which grabs the command-line arguments, determines
        which dates and times of data need to be processed, then processes the
        NARR data that falls between the specified dates

        Returns:
            SUCCESS/ERROR on successful or failed processing

        Notes:
            1. Datetimes for which the archive already has NARR data will be
            skipped for download and processing unless the overwrite flag is
            set (default off).

            2. When checking if a file is in the existing archive, the HGT
            variable file will be checked as all 3 variables for a given date
            should be present when archived.
    """
    # Get NARR config information
    c = AuxConfig()
    cfg = c.get_config('narr')
    start_date = datetime.strptime(cfg.get('narr_start_date'), '%Y-%m-%d')

    # Create a command line arugment parser
    description = ('Downloads ST auxiliary inputs, then archives them for'
                   ' future use. Dates must be the in the format: "YYYYMMDD"')
    args = System.get_command_line_arguments(description, start_date, False)

    # Setup logging
    System.setup_logging(args.debug)

    # Alert user if the overwrite flag is set
    if args.overwrite:
        logger.info('overwrite flag is set: Any existing archive data will '
                    'be overwritten by most current online archive data.')

    # Connect to the database
    try:
        con = cx_Oracle.connect(os.getenv('IAS_DB_COM'))
        dbh = con.cursor()
        logger.info('Connected to database successfully')
    except cx_Oracle.DatabaseError as e:
        logger.error('Could not connect to database: %s', e)
        sys.exit(ERROR)

    # Gather a list of the current archive files or create directory path
    archive = System.get_daily_archive_listing(args.start_date, args.end_date,
                                               cfg['narr_base_archive'],
                                               cfg['narr_archive_format'])

    # Determine the dates and times to be downloaded and processed
    dates = get_date_times(args.start_date, args.end_date,
                           cfg, args.overwrite, archive)

    # Download the files, archive them, and update the db archive records
    status = archive_aux_data(cfg, dates, args.overwrite, archive, con, dbh)

    # Close the db connection and return
    dbh.close()
    con.close()
    sys.exit(status)

if __name__ == '__main__':
    main()
